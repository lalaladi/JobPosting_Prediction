{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "oWgZayoqb8k6",
        "outputId": "60062373-d87e-427c-d60c-72a02d568c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "------------------------------------  ----------------------------------------  ------------------------\n",
            "ID                                    NAME                                      CREATED\n",
            "9b602fc8-d468-41cb-a19a-b140f78e4d00  Deployment Kelompok 2 - Capstone Project  2024-06-15T09:38:11.836Z\n",
            "6d8cddd1-c3b8-4c45-979d-5c8b81cd106b  Capstone                                  2024-06-14T09:50:19.401Z\n",
            "889c1e3c-484a-4a1a-8f4b-3de337b23f6d  Deploy                                    2024-06-11T00:42:01.970Z\n",
            "------------------------------------  ----------------------------------------  ------------------------\n"
          ]
        }
      ],
      "source": [
        "import re, string, nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as tf_hub\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "%pip install -U -q ibm-watson-machine-learning\n",
        "\n",
        "api_key='vD5MYd1qlPmuSio9ubmytdXmbUApziO74nS969CDjtAo'\n",
        "location = 'us-south'\n",
        "\n",
        "wml_credentials = {\n",
        "    \"apikey\": api_key,\n",
        "    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n",
        "}\n",
        "\n",
        "from ibm_watson_machine_learning import APIClient\n",
        "\n",
        "client = APIClient(wml_credentials)\n",
        "\n",
        "client.spaces.list(limit=5)\n",
        "\n",
        "space_id = '9b602fc8-d468-41cb-a19a-b140f78e4d00' #id dari space yg telah dibuatt\n",
        "\n",
        "client.set.default_space(space_id)\n",
        "\n",
        "new_text = \"Balanced Labs exists to improve the lives of accountants and their clients with intelligent and elegant tools. Weâ€™re an energetic team of ex-Xeroâ€™s, COOs, designers, and engineers based in San Francisco and Sydney.Â As our Communication Designer, you will help define our brand experience and how we communicate with our community.In this role, you will be responsible for our brand experience, and youâ€™ll work across our product and marketing design teams to define how our platform connects and resonates with our customers.You have a deep passion for communication design, and are willing to share your expertise with others.Balanced Labs is an Equal Opportunity Employer.\"\n",
        "\n",
        "max_features = 10000\n",
        "# Membuat Tokenizer\n",
        "t = Tokenizer(num_words = max_features)\n",
        "# fit tokenizer\n",
        "t.fit_on_texts(new_text)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "punctuation = list(string.punctuation)\n",
        "stop_words.update(punctuation)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "  # Case folding\n",
        "  text = text.lower()\n",
        "\n",
        "  # Newline removal (\\n)\n",
        "  text = re.sub(r\"\\\\n\", \" \",text)\n",
        "\n",
        "  # Whitespace removal\n",
        "  text = text.strip()\n",
        "\n",
        "  # URL removal\n",
        "  text = re.sub(r\"http\\S+\", \" \", text)\n",
        "  text = re.sub(r\"www.\\S+\", \" \", text)\n",
        "\n",
        "  # Tokenization\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Stopwords removal\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Lemmatization\n",
        "  tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "  # Combining Tokens\n",
        "  text = ' '.join(tokens)\n",
        "\n",
        "  return text\n",
        "\n",
        "text_prepoc = text_preprocessing(new_text)\n",
        "\n",
        "encoded_docs_testing = t.texts_to_sequences(text_prepoc)\n",
        "\n",
        "encoded_docs_testing = t.texts_to_sequences(text_prepoc)\n",
        "\n",
        "embedded_docs_testing = pad_sequences(encoded_docs_testing,padding='pre',maxlen=100000)\n",
        "\n",
        "testing_data = np.array(embedded_docs_testing)\n",
        "\n",
        "scoring_payload = {\"input_data\": [{\"values\": testing_data}]}\n",
        "deployment_id= '91213a22-e99b-4644-bf07-9501dbf54991'\n",
        "\n",
        "predictions = client.deployments.score(deployment_id, scoring_payload)\n",
        "\n",
        "scoring_response_json = predictions\n",
        "print(scoring_response_json)\n",
        "\n",
        "# Extracting and printing the prediction results\n",
        "prediction = scoring_response_json['predictions'][0]\n",
        "predicted_probabilities = prediction['values'][0][0]\n",
        "predicted_class = prediction['values'][0][1]\n",
        "\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Predicted probabilities: {predicted_probabilities}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXq36WTZU4-_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
